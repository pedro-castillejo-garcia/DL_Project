{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "2FJSNz8TmInv"
   },
   "outputs": [],
   "source": [
    "seq_len = 20\n",
    "batch_size = 50\n",
    "dropout = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "EJZVD2QSMB4t"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import math\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Paths to datasets\n",
    "file_paths = [\n",
    "    r\"./../data/raw/wind_speed_11_n.csv\",\n",
    "    r\"./../data/raw/wind_speed_13_n.csv\",\n",
    "    r\"./../data/raw/wind_speed_15_n.csv\",\n",
    "    r\"./../data/raw/wind_speed_17_n.csv\",\n",
    "    r\"./../data/raw/wind_speed_19_n.csv\"\n",
    "]\n",
    "\n",
    "# Load datasets\n",
    "datasets = [pd.read_csv(file) for file in file_paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zkjslkZyMJhJ"
   },
   "outputs": [],
   "source": [
    "#datasets = [dataset.iloc[:int(0.1 * len(dataset))] for dataset in datasets] #To get as much of the data as you need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a9ThE3NkYMdK"
   },
   "outputs": [],
   "source": [
    "# Define features and targets\n",
    "features = [\"Mx1\", \"Mx2\", \"Mx3\", \"My1\", \"My2\", \"My3\", \"Theta\", \"Vwx\", \"Vwy\", \"Vwz\",\n",
    "            \"beta1\", \"beta2\", \"beta3\", \"dbeta1\", \"dbeta2\", \"dbeta3\", \"omega_r\"]\n",
    "targets = [\"Mz1\", \"Mz2\", \"Mz3\"]\n",
    "\n",
    "train_data_x = []\n",
    "train_data_y = []\n",
    "val_data_x = []\n",
    "val_data_y = []\n",
    "test_data = {}\n",
    "\n",
    "# Collect all training features and targets\n",
    "all_train_features = []\n",
    "all_train_targets = []\n",
    "\n",
    "for dataset in datasets:\n",
    "    n = len(dataset)\n",
    "    train_end_idx = int(0.6 * n)\n",
    "    val_end_idx = int(0.8 * n)\n",
    "\n",
    "    # Sequential splits\n",
    "    train_segment = dataset.iloc[:train_end_idx]\n",
    "    val_segment = dataset.iloc[train_end_idx:val_end_idx]\n",
    "    test_segment = dataset.iloc[val_end_idx:]\n",
    "\n",
    "    # Append training features and targets to the list\n",
    "    all_train_features.append(train_segment[features].values)\n",
    "    all_train_targets.append(train_segment[targets].values)\n",
    "\n",
    "# Combine all training data into single arrays\n",
    "all_train_features = np.concatenate(all_train_features, axis=0)\n",
    "all_train_targets = np.concatenate(all_train_targets, axis=0)\n",
    "\n",
    "# Fit scalers on all training data\n",
    "scaler_x = MinMaxScaler()\n",
    "scaler_y = MinMaxScaler()\n",
    "\n",
    "scaler_x.fit(all_train_features)\n",
    "scaler_y.fit(all_train_targets)\n",
    "\n",
    "def create_sequences(data, targets, seq_len):\n",
    "    X_seq, y_seq = [], []\n",
    "    for i in range(len(data) - seq_len):\n",
    "        X_seq.append(data[i:i + seq_len])  # Add sequence of features\n",
    "        y_seq.append(targets[i + seq_len - 1])  # Predict the target at the end of the sequence\n",
    "    return np.array(X_seq), np.array(y_seq)\n",
    "\n",
    "# Split datasets\n",
    "i = 0\n",
    "for dataset in datasets:\n",
    "    n = len(dataset)\n",
    "    train_end_idx = int(0.6 * n)\n",
    "    val_end_idx = int(0.8 * n)\n",
    "\n",
    "    # Sequential splits\n",
    "    train_segment = dataset.iloc[:train_end_idx]\n",
    "    val_segment = dataset.iloc[train_end_idx:val_end_idx]\n",
    "    test_segment = dataset.iloc[val_end_idx:]\n",
    "\n",
    "    train_segment_x = scaler_x.transform(train_segment[features].values)\n",
    "    train_segment_y = scaler_y.transform(train_segment[targets].values)\n",
    "\n",
    "    val_segment_x = scaler_x.transform(val_segment[features].values)\n",
    "    val_segment_y = scaler_y.transform(val_segment[targets].values)\n",
    "\n",
    "    train_seq_x, train_seq_y = create_sequences(train_segment_x, train_segment_y, seq_len)\n",
    "    val_seq_x, val_seq_y = create_sequences(val_segment_x, val_segment_y, seq_len)\n",
    "\n",
    "    train_data_x.append(train_seq_x)\n",
    "    train_data_y.append(train_seq_y)\n",
    "    val_data_x.append(val_seq_x)\n",
    "    val_data_y.append(val_seq_y)\n",
    "    test_data[i] = test_segment\n",
    "    i += 1\n",
    "\n",
    "train_data_x = np.concatenate(train_data_x, axis=0)\n",
    "train_data_y = np.concatenate(train_data_y, axis=0)\n",
    "val_data_x = np.concatenate(val_data_x, axis=0)\n",
    "val_data_y = np.concatenate(val_data_y, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KKz9Y1RTYdvm"
   },
   "outputs": [],
   "source": [
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(train_data_x, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(train_data_y, dtype=torch.float32)\n",
    "X_val_tensor = torch.tensor(val_data_x, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(val_data_y, dtype=torch.float32)\n",
    "\n",
    "# Wrap tensors in DataLoader for batching\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vY5lTT99ey58"
   },
   "outputs": [],
   "source": [
    "# Define Feedforward Neural Network\n",
    "class FeedforwardNN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, dropout=0.1):\n",
    "        super(FeedforwardNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim * seq_len, 512)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.fc3 = nn.Linear(256, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)  # Flatten the input\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "input_dim = len(features)\n",
    "output_dim = len(targets)\n",
    "\n",
    "model = FeedforwardNN(input_dim=input_dim, output_dim=output_dim, dropout=dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BwZoRrmVe4gQ"
   },
   "outputs": [],
   "source": [
    "# Training setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 698
    },
    "id": "8ALY4Pow-r-a",
    "outputId": "c6a06877-29eb-4087-fc41-62b4e0cbe885"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initialize lists to store loss values\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "epochs = 50\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(X_batch)\n",
    "        loss = criterion(predictions, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    train_loss /= len(train_loader)\n",
    "    train_losses.append(train_loss)  # Store the training loss\n",
    "\n",
    "    # Evaluate on validation set\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in val_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            predictions = model(X_batch)\n",
    "            loss = criterion(predictions, y_batch)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    val_losses.append(val_loss)  # Store the validation loss\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "# Plot the losses\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(1, len(train_losses) + 1), train_losses, label='Training Loss')\n",
    "plt.plot(range(1, len(val_losses) + 1), val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss Over Epochs')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CegjnJM7YngG",
    "outputId": "56404197-9564-4807-e94d-94328513275a"
   },
   "outputs": [],
   "source": [
    "# Save the Model\n",
    "torch.save(model.state_dict(), \"ffn1.pth\")\n",
    "print(\"Model saved as ffn1.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "wSJCluDqCDgl",
    "outputId": "a35c1f78-1c91-40fc-e2df-17d8af515710"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "mse_per_dataset = {}\n",
    "total_mse = 0\n",
    "num_datasets = len(test_data)\n",
    "\n",
    "i = 0\n",
    "for test_df in test_data.values():\n",
    "\n",
    "  # Load the validation dataset\n",
    "  t_test = test_df[\"t\"].values  # Extract the 't' column for plotting\n",
    "  features_test = test_df.drop([\"t\", \"Mz1\", \"Mz2\", \"Mz3\"], axis=1).values  # Input features\n",
    "  targets_test = test_df[[\"Mz1\", \"Mz2\", \"Mz3\"]].values  # Ground truth for comparison\n",
    "\n",
    "  # Preprocess the validation data\n",
    "  features_test_scaled = scaler_x.transform(features_test)\n",
    "  targets_test_scaled = scaler_y.transform(targets_test)\n",
    "  X_test, y_test = create_sequences(features_test_scaled, targets_test_scaled, seq_len)  # Sequence preparation\n",
    "\n",
    "  # Generate predictions\n",
    "  model.eval()  # Set model to evaluation mode\n",
    "  with torch.no_grad():\n",
    "      X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "      X_test_tensor = X_test_tensor.to(device)\n",
    "      y_test_pred = model(X_test_tensor).cpu().numpy()  # Predictions from the model\n",
    "\n",
    "  # Align `t` for plotting\n",
    "  # Since the sequences shorten the dataset, align 't' with the predictions\n",
    "  t_test_aligned = t_test[seq_len:]  # Remove the first `total_len - 1` timesteps to match the predictions\n",
    "  target_test = targets_test[seq_len:]\n",
    "  # Plot results\n",
    "  inversed_pred = scaler_y.inverse_transform(y_test_pred)\n",
    "  inversed_true = scaler_y.inverse_transform(y_test)\n",
    "  mse = mean_squared_error(inversed_true, inversed_pred)\n",
    "  mse_per_dataset[f\"Dataset {i+1}\"] = mse\n",
    "  total_mse += mse\n",
    "\n",
    "    # get only the first 1600 examples for a more cohesive plot\n",
    "  max_examples = 1600\n",
    "  t_test_aligned_plot = t_test_aligned[:max_examples]\n",
    "  target_test_plot = target_test[:max_examples]\n",
    "  inversed_pred_plot = inversed_pred[:max_examples]\n",
    "\n",
    "  plt.figure(figsize=(12, 6))\n",
    "  for j, target_name in enumerate([\"Mz1\", \"Mz2\", \"Mz3\"]):\n",
    "      plt.plot(t_test_aligned_plot, target_test_plot[:, j], label=f\"Ground Truth {target_name} (Dotted)\", linestyle=\"dotted\")\n",
    "      plt.plot(t_test_aligned_plot, inversed_pred_plot[:, j], label=f\"Prediction {target_name}\")\n",
    "\n",
    "  plt.xlabel(\"Time (t)\")\n",
    "  plt.ylabel(\"Torque\")\n",
    "  plt.title(f\"Dataset {i+1}: Predictions vs Ground Truth for All Torques (Mz1, Mz2, Mz3)\")\n",
    "  plt.legend()\n",
    "  plt.grid()\n",
    "  plt.show()\n",
    "\n",
    "  i += 1\n",
    "\n",
    "print(\"Mean Squared Error (MSE) for each dataset:\")\n",
    "for dataset, mse in mse_per_dataset.items():\n",
    "    print(f\"{dataset}: {mse:.4f}\")\n",
    "\n",
    "average_mse = total_mse / num_datasets\n",
    "print(f\"\\nAverage MSE across all datasets: {average_mse:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
